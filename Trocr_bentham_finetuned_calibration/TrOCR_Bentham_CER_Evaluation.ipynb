{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# TrOCR Bentham Dataset Evaluation\n",
        "## Character Error Rate (CER) Evaluation of Fine-tuned TrOCR Model\n",
        "\n",
        "This notebook loads a pre-trained TrOCR model that has been fine-tuned on the Bentham dataset and evaluates its performance using Character Error Rate (CER) metrics.\n",
        "\n",
        "**Key Features:**\n",
        "- Skip training phase and directly load trained model\n",
        "- Comprehensive CER evaluation on test dataset\n",
        "- Sample predictions and analysis\n",
        "- Detailed performance metrics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets jiwer evaluate torch torchvision tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "imports"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n",
        "from tqdm.notebook import tqdm\n",
        "import evaluate\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "def load_iam_format_dataset(dataset_dir, split='train'):\n",
        "    \"\"\"\n",
        "    Load IAM format dataset (works with original IAM or converted Bentham)\n",
        "\n",
        "    Args:\n",
        "        dataset_dir: Path to dataset directory containing images/ and gt files\n",
        "        split: 'train', 'val', or 'all'\n",
        "    \"\"\"\n",
        "\n",
        "    images_dir = os.path.join(dataset_dir, 'images')\n",
        "\n",
        "    # Choose the appropriate ground truth file\n",
        "    if split == 'train':\n",
        "        gt_file = os.path.join(dataset_dir, 'gt_train.txt')\n",
        "    elif split == 'val':\n",
        "        gt_file = os.path.join(dataset_dir, 'gt_val.txt')\n",
        "    else:  # 'all' or fallback\n",
        "        gt_file = os.path.join(dataset_dir, 'gt.txt')\n",
        "\n",
        "    # Fallback to main gt.txt if split files don't exist\n",
        "    if not os.path.exists(gt_file):\n",
        "        gt_file = os.path.join(dataset_dir, 'gt.txt')\n",
        "        print(f\"Using fallback gt.txt for {split} split\")\n",
        "\n",
        "    data = []\n",
        "\n",
        "    with open(gt_file, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                # Split at first space: image_id text\n",
        "                parts = line.split(' ', 1)\n",
        "                if len(parts) == 2:\n",
        "                    image_id, text = parts\n",
        "\n",
        "                    # Find image file (try different extensions)\n",
        "                    image_path = None\n",
        "                    for ext in ['.png', '.jpg', '.jpeg', '.tiff', '.bmp']:\n",
        "                        potential_path = os.path.join(images_dir, f\"{image_id}{ext}\")\n",
        "                        if os.path.exists(potential_path):\n",
        "                            image_path = potential_path\n",
        "                            break\n",
        "\n",
        "                    if image_path:\n",
        "                        data.append({\n",
        "                            'image_id': image_id,\n",
        "                            'file_name': f\"{image_id}{os.path.splitext(image_path)[1]}\",\n",
        "                            'text': text,\n",
        "                            'image_path': image_path\n",
        "                        })\n",
        "                    else:\n",
        "                        print(f\"Warning: Image not found for {image_id}\")\n",
        "\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "print('Libraries imported successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading datasets...\n",
            "Training samples: 18356\n",
            "Test samples: 4590\n",
            "\n",
            "Sample from test dataset:\n",
            "         image_id           file_name  \\\n",
            "0  bentham_004262  bentham_004262.png   \n",
            "1  bentham_008658  bentham_008658.png   \n",
            "2  bentham_018238  bentham_018238.png   \n",
            "3  bentham_009145  bentham_009145.png   \n",
            "4  bentham_014041  bentham_014041.png   \n",
            "\n",
            "                                                text  \\\n",
            "0       in general , or to the party in particular .   \n",
            "1  upon specific acts  :  upon the wounding  , st...   \n",
            "2  it necessary , and thereby preserve them from be-   \n",
            "3  it remained a doubt whether it could be accomp...   \n",
            "4                                         envelopped   \n",
            "\n",
            "                                          image_path  \n",
            "0  B:\\SaravanaVel\\Documents\\Academics MCA\\Project...  \n",
            "1  B:\\SaravanaVel\\Documents\\Academics MCA\\Project...  \n",
            "2  B:\\SaravanaVel\\Documents\\Academics MCA\\Project...  \n",
            "3  B:\\SaravanaVel\\Documents\\Academics MCA\\Project...  \n",
            "4  B:\\SaravanaVel\\Documents\\Academics MCA\\Project...  \n",
            "\n",
            "First test image exists: True\n",
            "Sample text: in general , or to the party in particular .\n",
            "Sample image path: B:\\SaravanaVel\\Documents\\Academics MCA\\Project\\Final Code\\Final Code\\Dataset\\bentham_iam_format\\images\\bentham_004262.png\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "dataset_dir = 'B:\\\\SaravanaVel\\\\Documents\\\\Academics MCA\\\\Project\\\\Final Code\\\\Final Code\\\\Dataset\\\\bentham_iam_format'\n",
        "\n",
        "# Load training and test data\n",
        "print('Loading datasets...')\n",
        "train_df = load_iam_format_dataset(dataset_dir, split='train')\n",
        "test_df = load_iam_format_dataset(dataset_dir, split='val')  # Using val as test\n",
        "\n",
        "print(f'Training samples: {len(train_df)}')\n",
        "print(f'Test samples: {len(test_df)}')\n",
        "\n",
        "# Display sample data\n",
        "print('\\nSample from test dataset:')\n",
        "print(test_df.head())\n",
        "\n",
        "# Check if images exist\n",
        "print(f'\\nFirst test image exists: {os.path.exists(test_df.iloc[0][\"image_path\"])}')\n",
        "print(f'Sample text: {test_df.iloc[0][\"text\"]}')\n",
        "print(f'Sample image path: {test_df.iloc[0][\"image_path\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "dataset_class"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset class defined successfully\n"
          ]
        }
      ],
      "source": [
        "class BenthamDataset(Dataset):\n",
        "    def __init__(self, df, processor, max_target_length=128):\n",
        "        self.df = df\n",
        "        self.processor = processor\n",
        "        self.max_target_length = max_target_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get image path and text\n",
        "        image_path = self.df.loc[idx, 'image_path']\n",
        "        text = self.df.loc[idx, 'text']\n",
        "\n",
        "        # Load and process image (resize + normalize)\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n",
        "\n",
        "        # Process text (tokenize)\n",
        "        labels = self.processor.tokenizer(text,\n",
        "                                        padding=\"max_length\",\n",
        "                                        max_length=self.max_target_length,\n",
        "                                        truncation=True,\n",
        "                                        return_tensors=\"pt\").input_ids\n",
        "\n",
        "        # Important: make sure PAD tokens are ignored by loss function\n",
        "        labels[labels == self.processor.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values.squeeze(),\n",
        "            \"labels\": labels.squeeze(),\n",
        "            \"text\": text,  # Keep original text for evaluation\n",
        "            \"image_path\": image_path\n",
        "        }\n",
        "\n",
        "print('Dataset class defined successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n",
            "Loading model from: B:\\\\SaravanaVel\\\\Documents\\\\Academics MCA\\\\Project\\\\Final Code\\\\Final Code\\\\Models\\\\fine_tuned\n",
            " Model and processor loaded successfully\n",
            "Model config: VisionEncoderDecoderConfig {\n",
            "  \"architectures\": [\n",
            "    \"VisionEncoderDecoderModel\"\n",
            "  ],\n",
            "  \"decoder\": {\n",
            "    \"activation_dropout\": 0.0,\n",
            "    \"activation_function\": \"gelu\",\n",
            "    \"add_cross_attention\": true,\n",
            "    \"attention_dropout\": 0.0,\n",
            "    \"classifier_dropout\": 0.0,\n",
            "    \"cross_attention_hidden_size\": 768,\n",
            "    \"d_model\": 1024,\n",
            "    \"decoder_attention_heads\": 16,\n",
            "    \"decoder_ffn_dim\": 4096,\n",
            "    \"decoder_layerdrop\": 0.0,\n",
            "    \"decoder_layers\": 12,\n",
            "    \"dropout\": 0.1,\n",
            "    \"dtype\": \"float32\",\n",
            "    \"init_std\": 0.02,\n",
            "    \"is_decoder\": true,\n",
            "    \"layernorm_embedding\": true,\n",
            "    \"max_position_embeddings\": 512,\n",
            "    \"model_type\": \"trocr\",\n",
            "    \"scale_embedding\": false,\n",
            "    \"use_cache\": false,\n",
            "    \"use_learned_position_embeddings\": true,\n",
            "    \"vocab_size\": 50265\n",
            "  },\n",
            "  \"dtype\": \"float32\",\n",
            "  \"encoder\": {\n",
            "    \"attention_probs_dropout_prob\": 0.0,\n",
            "    \"dtype\": \"float32\",\n",
            "    \"encoder_stride\": 16,\n",
            "    \"hidden_act\": \"gelu\",\n",
            "    \"hidden_dropout_prob\": 0.0,\n",
            "    \"hidden_size\": 768,\n",
            "    \"image_size\": 384,\n",
            "    \"initializer_range\": 0.02,\n",
            "    \"intermediate_size\": 3072,\n",
            "    \"layer_norm_eps\": 1e-12,\n",
            "    \"model_type\": \"vit\",\n",
            "    \"num_attention_heads\": 12,\n",
            "    \"num_channels\": 3,\n",
            "    \"num_hidden_layers\": 12,\n",
            "    \"patch_size\": 16,\n",
            "    \"pooler_act\": \"tanh\",\n",
            "    \"pooler_output_size\": 768,\n",
            "    \"qkv_bias\": false\n",
            "  },\n",
            "  \"is_encoder_decoder\": true,\n",
            "  \"model_type\": \"vision-encoder-decoder\",\n",
            "  \"processor_class\": \"TrOCRProcessor\",\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"transformers_version\": \"4.56.1\"\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the fine-tuned model and processor\n",
        "# Update this path to point to your trained model directory\n",
        "MODEL_PATH = r\"B:\\\\SaravanaVel\\\\Documents\\\\Academics MCA\\\\Project\\\\Final Code\\\\Final Code\\\\Models\\\\fine_tuned\"  # Change this to your model path\n",
        "\n",
        "print(f'Loading model from: {MODEL_PATH}')\n",
        "\n",
        "try:\n",
        "    # Load processor and model\n",
        "    processor = TrOCRProcessor.from_pretrained(MODEL_PATH)\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(MODEL_PATH)\n",
        "    model.to(device)\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    \n",
        "    print(' Model and processor loaded successfully')\n",
        "    print(f'Model config: {model.config}')\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f' Error loading model: {e}')\n",
        "    print('\\nTrying to load from alternative paths...')\n",
        "    \n",
        "    # Try alternative paths\n",
        "    alternative_paths = [\n",
        "        \"bentham-trocr-epoch-3\",\n",
        "        \"bentham-trocr-epoch-2\",\n",
        "        \"bentham-trocr-epoch-1\"\n",
        "    ]\n",
        "    \n",
        "    for alt_path in alternative_paths:\n",
        "        try:\n",
        "            print(f'Trying: {alt_path}')\n",
        "            processor = TrOCRProcessor.from_pretrained(alt_path)\n",
        "            model = VisionEncoderDecoderModel.from_pretrained(alt_path)\n",
        "            model.to(device)\n",
        "            model.eval()\n",
        "            MODEL_PATH = alt_path\n",
        "            print(f'✓ Successfully loaded from: {alt_path}')\n",
        "            break\n",
        "        except:\n",
        "            continue\n",
        "    else:\n",
        "        print(' Could not find any trained model. Please check the model path.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "create_dataset"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created test dataset with 4590 samples\n",
            "Test batches: 574\n"
          ]
        }
      ],
      "source": [
        "# Create test dataset\n",
        "test_dataset = BenthamDataset(df=test_df, processor=processor)\n",
        "\n",
        "print(f\"Created test dataset with {len(test_dataset)} samples\")\n",
        "\n",
        "# Create data loader for evaluation\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
        "\n",
        "print(f\"Test batches: {len(test_dataloader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "cer_metric"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c2f9dbb15be04d7daab2a52cfcaeb7ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation metrics loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# Load CER evaluation metric\n",
        "cer_metric = evaluate.load(\"cer\")\n",
        "wer_metric = evaluate.load(\"wer\")\n",
        "\n",
        "print('Evaluation metrics loaded successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "cer_function"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CER computation functions defined successfully\n"
          ]
        }
      ],
      "source": [
        "def compute_cer(pred_ids, label_ids):\n",
        "    \"\"\"Compute Character Error Rate\"\"\"\n",
        "    # Decode predictions\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "\n",
        "    # Decode labels (replace -100 with pad token)\n",
        "    label_ids_copy = label_ids.clone()\n",
        "    label_ids_copy[label_ids_copy == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = processor.batch_decode(label_ids_copy, skip_special_tokens=True)\n",
        "\n",
        "    # Compute CER\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    return cer, pred_str, label_str\n",
        "\n",
        "def compute_wer(pred_str, label_str):\n",
        "    \"\"\"Compute Word Error Rate\"\"\"\n",
        "    wer = wer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    return wer\n",
        "\n",
        "def predict_single_image(image_path, model, processor, device):\n",
        "    \"\"\"Predict text from a single image\"\"\"\n",
        "    # Load and process image\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    pixel_values = processor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
        "    \n",
        "    # Generate prediction\n",
        "    with torch.no_grad():\n",
        "        generated_ids = model.generate(pixel_values)\n",
        "        predicted_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    \n",
        "    return predicted_text\n",
        "\n",
        "print('CER computation functions defined successfully')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sample_predictions"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing model on sample images...\n",
            "\n",
            "Sample 1:\n",
            "Image: bentham_004262.png\n",
            "Ground Truth: 'in general , or to the party in particular .'\n",
            "Prediction:   'in general , or to the party in particular .'\n",
            "CER: 0.0000\n",
            "WER: 0.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Sample 2:\n",
            "Image: bentham_008658.png\n",
            "Ground Truth: 'upon specific acts  :  upon the wounding  , starving or poi'\n",
            "Prediction:   'upon specific acts , upon the wounding , starving or poor-'\n",
            "CER: 0.1186\n",
            "WER: 0.1818\n",
            "--------------------------------------------------------------------------------\n",
            "Sample 3:\n",
            "Image: bentham_018238.png\n",
            "Ground Truth: 'it necessary , and thereby preserve them from be-'\n",
            "Prediction:   'it necessary , and thereby pressure them from her'\n",
            "CER: 0.1020\n",
            "WER: 0.2222\n",
            "--------------------------------------------------------------------------------\n",
            "Sample 4:\n",
            "Image: bentham_009145.png\n",
            "Ground Truth: 'it remained a doubt whether it could be accomplished to any considerable extent .'\n",
            "Prediction:   'It remained a doubt whether it could be accomplished to carry considerable extent'\n",
            "CER: 0.0741\n",
            "WER: 0.2143\n",
            "--------------------------------------------------------------------------------\n",
            "Sample 5:\n",
            "Image: bentham_014041.png\n",
            "Ground Truth: 'envelopped'\n",
            "Prediction:   'enveloped'\n",
            "CER: 0.1000\n",
            "WER: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Sample Results Summary:\n",
            "Average CER on 5 samples: 0.0790\n",
            "Average WER on 5 samples: 0.3237\n"
          ]
        }
      ],
      "source": [
        "# Test the model on a few sample images first\n",
        "print(\"Testing model on sample images...\\n\")\n",
        "\n",
        "num_samples = 5\n",
        "sample_results = []\n",
        "\n",
        "for i in range(min(num_samples, len(test_df))):\n",
        "    image_path = test_df.iloc[i]['image_path']\n",
        "    ground_truth = test_df.iloc[i]['text']\n",
        "    \n",
        "    # Make prediction\n",
        "    predicted_text = predict_single_image(image_path, model, processor, device)\n",
        "    \n",
        "    # Compute individual CER\n",
        "    individual_cer = cer_metric.compute(predictions=[predicted_text], references=[ground_truth])\n",
        "    individual_wer = wer_metric.compute(predictions=[predicted_text], references=[ground_truth])\n",
        "    \n",
        "    sample_results.append({\n",
        "        'image_path': image_path,\n",
        "        'ground_truth': ground_truth,\n",
        "        'prediction': predicted_text,\n",
        "        'cer': individual_cer,\n",
        "        'wer': individual_wer\n",
        "    })\n",
        "    \n",
        "    print(f\"Sample {i+1}:\")\n",
        "    print(f\"Image: {os.path.basename(image_path)}\")\n",
        "    print(f\"Ground Truth: '{ground_truth}'\")\n",
        "    print(f\"Prediction:   '{predicted_text}'\")\n",
        "    print(f\"CER: {individual_cer:.4f}\")\n",
        "    print(f\"WER: {individual_wer:.4f}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "# Calculate average for samples\n",
        "avg_sample_cer = np.mean([r['cer'] for r in sample_results])\n",
        "avg_sample_wer = np.mean([r['wer'] for r in sample_results])\n",
        "\n",
        "print(f\"\\nSample Results Summary:\")\n",
        "print(f\"Average CER on {num_samples} samples: {avg_sample_cer:.4f}\")\n",
        "print(f\"Average WER on {num_samples} samples: {avg_sample_wer:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "full_evaluation"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting comprehensive evaluation on full test dataset...\n",
            "Total test samples: 4590\n",
            "Total batches: 574\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b532f261439546f0bb00e53c5ad45ea5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/574 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Comprehensive evaluation on the full test dataset\n",
        "print(\"Starting comprehensive evaluation on full test dataset...\")\n",
        "print(f\"Total test samples: {len(test_dataset)}\")\n",
        "print(f\"Total batches: {len(test_dataloader)}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "model.eval()\n",
        "total_cer = 0.0\n",
        "total_wer = 0.0\n",
        "num_batches = 0\n",
        "all_predictions = []\n",
        "all_ground_truths = []\n",
        "batch_results = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(tqdm(test_dataloader, desc=\"Evaluating\")):\n",
        "        # Move pixel values to device\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        \n",
        "        # Generate predictions\n",
        "        outputs = model.generate(pixel_values)\n",
        "        \n",
        "        # Compute CER for this batch\n",
        "        cer, pred_str, label_str = compute_cer(pred_ids=outputs, label_ids=batch[\"labels\"])\n",
        "        wer = compute_wer(pred_str, label_str)\n",
        "        \n",
        "        # Store results\n",
        "        total_cer += cer\n",
        "        total_wer += wer\n",
        "        num_batches += 1\n",
        "        \n",
        "        # Collect all predictions and ground truths\n",
        "        all_predictions.extend(pred_str)\n",
        "        all_ground_truths.extend(label_str)\n",
        "        \n",
        "        # Store batch results\n",
        "        batch_results.append({\n",
        "            'batch_idx': batch_idx,\n",
        "            'batch_size': len(pred_str),\n",
        "            'cer': cer,\n",
        "            'wer': wer,\n",
        "            'predictions': pred_str,\n",
        "            'ground_truths': label_str\n",
        "        })\n",
        "        \n",
        "        # Print progress every 10 batches\n",
        "        if (batch_idx + 1) % 10 == 0:\n",
        "            avg_cer_so_far = total_cer / num_batches\n",
        "            avg_wer_so_far = total_wer / num_batches\n",
        "            print(f\"Processed {batch_idx + 1}/{len(test_dataloader)} batches | \"\n",
        "                  f\"Running CER: {avg_cer_so_far:.4f} | \"\n",
        "                  f\"Running WER: {avg_wer_so_far:.4f}\")\n",
        "\n",
        "# Calculate final metrics\n",
        "final_cer = total_cer / num_batches\n",
        "final_wer = total_wer / num_batches\n",
        "\n",
        "# Also compute overall CER/WER on all predictions at once\n",
        "overall_cer = cer_metric.compute(predictions=all_predictions, references=all_ground_truths)\n",
        "overall_wer = wer_metric.compute(predictions=all_predictions, references=all_ground_truths)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model Path: {MODEL_PATH}\")\n",
        "print(f\"Test Dataset Size: {len(test_dataset)} samples\")\n",
        "print(f\"Number of Batches: {num_batches}\")\n",
        "print(\"\\nBatch-averaged Metrics:\")\n",
        "print(f\"Final CER: {final_cer:.4f} ({final_cer*100:.2f}%)\")\n",
        "print(f\"Final WER: {final_wer:.4f} ({final_wer*100:.2f}%)\")\n",
        "print(\"\\nOverall Metrics (all predictions):\")\n",
        "print(f\"Overall CER: {overall_cer:.4f} ({overall_cer*100:.2f}%)\")\n",
        "print(f\"Overall WER: {overall_wer:.4f} ({overall_wer*100:.2f}%)\")\n",
        "print(f\"Character Accuracy: {(1 - overall_cer) * 100:.2f}%\")\n",
        "print(f\"Word Accuracy: {(1 - overall_wer) * 100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "analysis"
      },
      "outputs": [],
      "source": [
        "# Detailed analysis of results\n",
        "print(\"DETAILED ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# CER distribution analysis\n",
        "individual_cers = []\n",
        "individual_wers = []\n",
        "\n",
        "for pred, gt in zip(all_predictions, all_ground_truths):\n",
        "    cer = cer_metric.compute(predictions=[pred], references=[gt])\n",
        "    wer = wer_metric.compute(predictions=[pred], references=[gt])\n",
        "    individual_cers.append(cer)\n",
        "    individual_wers.append(wer)\n",
        "\n",
        "individual_cers = np.array(individual_cers)\n",
        "individual_wers = np.array(individual_wers)\n",
        "\n",
        "print(f\"\\nCER Statistics:\")\n",
        "print(f\"Mean CER: {np.mean(individual_cers):.4f}\")\n",
        "print(f\"Median CER: {np.median(individual_cers):.4f}\")\n",
        "print(f\"Std CER: {np.std(individual_cers):.4f}\")\n",
        "print(f\"Min CER: {np.min(individual_cers):.4f}\")\n",
        "print(f\"Max CER: {np.max(individual_cers):.4f}\")\n",
        "\n",
        "print(f\"\\nWER Statistics:\")\n",
        "print(f\"Mean WER: {np.mean(individual_wers):.4f}\")\n",
        "print(f\"Median WER: {np.median(individual_wers):.4f}\")\n",
        "print(f\"Std WER: {np.std(individual_wers):.4f}\")\n",
        "print(f\"Min WER: {np.min(individual_wers):.4f}\")\n",
        "print(f\"Max WER: {np.max(individual_wers):.4f}\")\n",
        "\n",
        "# Perfect predictions analysis\n",
        "perfect_predictions = sum(1 for cer in individual_cers if cer == 0.0)\n",
        "perfect_word_predictions = sum(1 for wer in individual_wers if wer == 0.0)\n",
        "\n",
        "print(f\"\\nPerfect Predictions:\")\n",
        "print(f\"Perfect character predictions: {perfect_predictions}/{len(individual_cers)} ({perfect_predictions/len(individual_cers)*100:.2f}%)\")\n",
        "print(f\"Perfect word predictions: {perfect_word_predictions}/{len(individual_wers)} ({perfect_word_predictions/len(individual_wers)*100:.2f}%)\")\n",
        "\n",
        "# High error predictions analysis\n",
        "high_cer_threshold = 0.5  # 50% character error\n",
        "high_cer_count = sum(1 for cer in individual_cers if cer > high_cer_threshold)\n",
        "\n",
        "print(f\"\\nHigh Error Analysis:\")\n",
        "print(f\"Predictions with CER > {high_cer_threshold}: {high_cer_count}/{len(individual_cers)} ({high_cer_count/len(individual_cers)*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "best_worst"
      },
      "outputs": [],
      "source": [
        "# Show best and worst predictions\n",
        "print(\"BEST AND WORST PREDICTIONS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get indices sorted by CER\n",
        "cer_indices = np.argsort(individual_cers)\n",
        "\n",
        "print(\"\\n BEST PREDICTIONS (Lowest CER):\")\n",
        "print(\"-\" * 40)\n",
        "for i in range(min(5, len(cer_indices))):\n",
        "    idx = cer_indices[i]\n",
        "    print(f\"\\nRank {i+1} (CER: {individual_cers[idx]:.4f}):\")\n",
        "    print(f\"Ground Truth: '{all_ground_truths[idx]}'\")\n",
        "    print(f\"Prediction:   '{all_predictions[idx]}'\")\n",
        "\n",
        "print(\"\\n\\n WORST PREDICTIONS (Highest CER):\")\n",
        "print(\"-\" * 40)\n",
        "for i in range(min(5, len(cer_indices))):\n",
        "    idx = cer_indices[-(i+1)]  # Get from the end\n",
        "    print(f\"\\nRank {i+1} (CER: {individual_cers[idx]:.4f}):\")\n",
        "    print(f\"Ground Truth: '{all_ground_truths[idx]}'\")\n",
        "    print(f\"Prediction:   '{all_predictions[idx]}'\")\n",
        "    \n",
        "    # Try to identify common error patterns\n",
        "    gt_len = len(all_ground_truths[idx])\n",
        "    pred_len = len(all_predictions[idx])\n",
        "    print(f\"Length: GT={gt_len}, Pred={pred_len}, Diff={pred_len-gt_len}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "comparison"
      },
      "outputs": [],
      "source": [
        "# Performance comparison with benchmarks\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Benchmark comparisons based on literature\n",
        "benchmarks = {\n",
        "    \"TrOCR Base (IAM)\": {\"cer\": 0.03, \"source\": \"Original TrOCR paper\"},\n",
        "    \"TrOCR Base (handwritten)\": {\"cer\": 0.062, \"source\": \"Fine-tuning example\"},\n",
        "    \"Our Fine-tuned Model\": {\"cer\": overall_cer, \"source\": \"Current evaluation\"}\n",
        "}\n",
        "\n",
        "print(\"\\nCharacter Error Rate Comparison:\")\n",
        "print(\"-\" * 50)\n",
        "for model, metrics in benchmarks.items():\n",
        "    print(f\"{model:<25}: {metrics['cer']:.4f} ({metrics['cer']*100:.2f}%) - {metrics['source']}\")\n",
        "\n",
        "print(\"\\n Model Performance Summary:\")\n",
        "print(f\"- Dataset: Bentham historical handwriting dataset\")\n",
        "print(f\"- Test samples: {len(test_dataset)}\")\n",
        "print(f\"- Character Error Rate: {overall_cer:.4f} ({overall_cer*100:.2f}%)\")\n",
        "print(f\"- Word Error Rate: {overall_wer:.4f} ({overall_wer*100:.2f}%)\")\n",
        "print(f\"- Character Accuracy: {(1-overall_cer)*100:.2f}%\")\n",
        "print(f\"- Word Accuracy: {(1-overall_wer)*100:.2f}%\")\n",
        "print(f\"- Perfect character predictions: {perfect_predictions/len(individual_cers)*100:.2f}%\")\n",
        "print(f\"- Perfect word predictions: {perfect_word_predictions/len(individual_wers)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_results"
      },
      "outputs": [],
      "source": [
        "# Save detailed results to files\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Prepare results dictionary\n",
        "results = {\n",
        "    \"model_path\": MODEL_PATH,\n",
        "    \"dataset_size\": len(test_dataset),\n",
        "    \"batch_averaged_metrics\": {\n",
        "        \"cer\": float(final_cer),\n",
        "        \"wer\": float(final_wer)\n",
        "    },\n",
        "    \"overall_metrics\": {\n",
        "        \"cer\": float(overall_cer),\n",
        "        \"wer\": float(overall_wer),\n",
        "        \"character_accuracy\": float((1 - overall_cer) * 100),\n",
        "        \"word_accuracy\": float((1 - overall_wer) * 100)\n",
        "    },\n",
        "    \"statistics\": {\n",
        "        \"cer_mean\": float(np.mean(individual_cers)),\n",
        "        \"cer_median\": float(np.median(individual_cers)),\n",
        "        \"cer_std\": float(np.std(individual_cers)),\n",
        "        \"cer_min\": float(np.min(individual_cers)),\n",
        "        \"cer_max\": float(np.max(individual_cers)),\n",
        "        \"perfect_char_predictions\": int(perfect_predictions),\n",
        "        \"perfect_word_predictions\": int(perfect_word_predictions),\n",
        "        \"perfect_char_percentage\": float(perfect_predictions/len(individual_cers)*100),\n",
        "        \"perfect_word_percentage\": float(perfect_word_predictions/len(individual_wers)*100)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save results as JSON\n",
        "results_filename = f\"evaluation_results_{MODEL_PATH.replace('/', '_')}.json\"\n",
        "with open(results_filename, 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"✓ Results saved to: {results_filename}\")\n",
        "\n",
        "# Save detailed predictions as CSV\n",
        "detailed_results = []\n",
        "for i, (pred, gt, cer, wer) in enumerate(zip(all_predictions, all_ground_truths, individual_cers, individual_wers)):\n",
        "    detailed_results.append({\n",
        "        'sample_id': i,\n",
        "        'ground_truth': gt,\n",
        "        'prediction': pred,\n",
        "        'cer': cer,\n",
        "        'wer': wer,\n",
        "        'ground_truth_length': len(gt),\n",
        "        'prediction_length': len(pred)\n",
        "    })\n",
        "\n",
        "detailed_df = pd.DataFrame(detailed_results)\n",
        "detailed_filename = f\"detailed_predictions_{MODEL_PATH.replace('/', '_')}.csv\"\n",
        "detailed_df.to_csv(detailed_filename, index=False)\n",
        "\n",
        "print(f\"✓ Detailed predictions saved to: {detailed_filename}\")\n",
        "print(f\"✓ Total files saved: 2\")\n",
        "\n",
        "# Display final summary\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EVALUATION COMPLETE!\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Final CER: {overall_cer:.4f} ({overall_cer*100:.2f}%)\")\n",
        "print(f\"Final WER: {overall_wer:.4f} ({overall_wer*100:.2f}%)\")\n",
        "print(f\"Character Accuracy: {(1-overall_cer)*100:.2f}%\")\n",
        "print(f\"Word Accuracy: {(1-overall_wer)*100:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
